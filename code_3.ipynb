{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code.ipynb\n",
      "index.html\n",
      "README.md\n",
      "code-checkpoint.ipynb\n",
      "1sherman_h_d_zhu_j_auth_service_productivity_management_impro.pdf\n",
      "data-2018-09-24.csv\n",
      "example.csv\n",
      "output.csv\n"
     ]
    }
   ],
   "source": [
    "#Imnporting libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_slsqp\n",
    "import os\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "#Listing the existing files in the dir\n",
    "for root, dirs, files in os.walk(\".\"):  \n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading df\n",
    "df = pd.read_csv('./files/data-2018-09-24.csv')\n",
    "df = df.drop(df.loc[:,['name', 'pft']].head(0).columns, axis=1)\n",
    "inpt_df = df.iloc[:, 1:6]\n",
    "inpt_arr = np.array(inpt_df)\n",
    "outpt_df = df.iloc[:, 6:]\n",
    "outpt_arr = np.array(outpt_df)\n",
    "comp = np.array(df.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firm</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>86.13</td>\n",
       "      <td>16.24</td>\n",
       "      <td>48.21</td>\n",
       "      <td>49.69</td>\n",
       "      <td>9</td>\n",
       "      <td>54.53</td>\n",
       "      <td>58.98</td>\n",
       "      <td>38.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>29.26</td>\n",
       "      <td>10.24</td>\n",
       "      <td>41.96</td>\n",
       "      <td>40.65</td>\n",
       "      <td>5</td>\n",
       "      <td>24.69</td>\n",
       "      <td>33.89</td>\n",
       "      <td>26.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>43.12</td>\n",
       "      <td>11.31</td>\n",
       "      <td>38.19</td>\n",
       "      <td>35.03</td>\n",
       "      <td>9</td>\n",
       "      <td>36.41</td>\n",
       "      <td>40.62</td>\n",
       "      <td>28.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>24.96</td>\n",
       "      <td>6.14</td>\n",
       "      <td>24.81</td>\n",
       "      <td>25.15</td>\n",
       "      <td>7</td>\n",
       "      <td>14.94</td>\n",
       "      <td>17.58</td>\n",
       "      <td>16.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>11.62</td>\n",
       "      <td>2.21</td>\n",
       "      <td>6.85</td>\n",
       "      <td>6.37</td>\n",
       "      <td>4</td>\n",
       "      <td>7.81</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>11.88</td>\n",
       "      <td>4.97</td>\n",
       "      <td>18.73</td>\n",
       "      <td>18.04</td>\n",
       "      <td>4</td>\n",
       "      <td>12.59</td>\n",
       "      <td>16.85</td>\n",
       "      <td>12.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>G</td>\n",
       "      <td>32.64</td>\n",
       "      <td>6.88</td>\n",
       "      <td>28.10</td>\n",
       "      <td>25.45</td>\n",
       "      <td>7</td>\n",
       "      <td>17.06</td>\n",
       "      <td>16.99</td>\n",
       "      <td>17.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H</td>\n",
       "      <td>20.79</td>\n",
       "      <td>12.97</td>\n",
       "      <td>54.85</td>\n",
       "      <td>52.07</td>\n",
       "      <td>8</td>\n",
       "      <td>20.29</td>\n",
       "      <td>30.64</td>\n",
       "      <td>33.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I</td>\n",
       "      <td>34.40</td>\n",
       "      <td>11.04</td>\n",
       "      <td>38.16</td>\n",
       "      <td>42.40</td>\n",
       "      <td>8</td>\n",
       "      <td>26.13</td>\n",
       "      <td>29.80</td>\n",
       "      <td>26.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>J</td>\n",
       "      <td>61.74</td>\n",
       "      <td>14.50</td>\n",
       "      <td>49.09</td>\n",
       "      <td>42.92</td>\n",
       "      <td>9</td>\n",
       "      <td>46.42</td>\n",
       "      <td>51.59</td>\n",
       "      <td>35.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>K</td>\n",
       "      <td>52.92</td>\n",
       "      <td>11.67</td>\n",
       "      <td>39.48</td>\n",
       "      <td>39.64</td>\n",
       "      <td>5</td>\n",
       "      <td>39.80</td>\n",
       "      <td>37.73</td>\n",
       "      <td>30.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L</td>\n",
       "      <td>36.00</td>\n",
       "      <td>10.15</td>\n",
       "      <td>37.80</td>\n",
       "      <td>39.52</td>\n",
       "      <td>5</td>\n",
       "      <td>37.84</td>\n",
       "      <td>47.85</td>\n",
       "      <td>25.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M</td>\n",
       "      <td>39.20</td>\n",
       "      <td>10.80</td>\n",
       "      <td>41.04</td>\n",
       "      <td>41.12</td>\n",
       "      <td>7</td>\n",
       "      <td>26.48</td>\n",
       "      <td>31.36</td>\n",
       "      <td>26.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>N</td>\n",
       "      <td>14.60</td>\n",
       "      <td>2.88</td>\n",
       "      <td>9.64</td>\n",
       "      <td>11.14</td>\n",
       "      <td>3</td>\n",
       "      <td>10.31</td>\n",
       "      <td>10.86</td>\n",
       "      <td>7.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>O</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5.42</td>\n",
       "      <td>21.45</td>\n",
       "      <td>17.27</td>\n",
       "      <td>5</td>\n",
       "      <td>14.39</td>\n",
       "      <td>18.30</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P</td>\n",
       "      <td>27.25</td>\n",
       "      <td>14.17</td>\n",
       "      <td>56.46</td>\n",
       "      <td>55.26</td>\n",
       "      <td>9</td>\n",
       "      <td>32.94</td>\n",
       "      <td>36.03</td>\n",
       "      <td>38.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Q</td>\n",
       "      <td>22.63</td>\n",
       "      <td>4.43</td>\n",
       "      <td>15.40</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2</td>\n",
       "      <td>17.25</td>\n",
       "      <td>20.80</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>R</td>\n",
       "      <td>28.00</td>\n",
       "      <td>7.61</td>\n",
       "      <td>28.73</td>\n",
       "      <td>27.04</td>\n",
       "      <td>9</td>\n",
       "      <td>27.55</td>\n",
       "      <td>38.19</td>\n",
       "      <td>20.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>S</td>\n",
       "      <td>53.56</td>\n",
       "      <td>13.70</td>\n",
       "      <td>53.04</td>\n",
       "      <td>49.85</td>\n",
       "      <td>7</td>\n",
       "      <td>41.12</td>\n",
       "      <td>43.80</td>\n",
       "      <td>36.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>T</td>\n",
       "      <td>25.42</td>\n",
       "      <td>9.05</td>\n",
       "      <td>29.69</td>\n",
       "      <td>31.74</td>\n",
       "      <td>4</td>\n",
       "      <td>29.43</td>\n",
       "      <td>42.63</td>\n",
       "      <td>23.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>U</td>\n",
       "      <td>31.57</td>\n",
       "      <td>10.08</td>\n",
       "      <td>39.34</td>\n",
       "      <td>40.57</td>\n",
       "      <td>6</td>\n",
       "      <td>37.46</td>\n",
       "      <td>51.02</td>\n",
       "      <td>27.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>V</td>\n",
       "      <td>16.34</td>\n",
       "      <td>5.84</td>\n",
       "      <td>20.89</td>\n",
       "      <td>22.10</td>\n",
       "      <td>4</td>\n",
       "      <td>19.40</td>\n",
       "      <td>25.18</td>\n",
       "      <td>16.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>W</td>\n",
       "      <td>44.28</td>\n",
       "      <td>14.14</td>\n",
       "      <td>56.70</td>\n",
       "      <td>52.27</td>\n",
       "      <td>11</td>\n",
       "      <td>39.88</td>\n",
       "      <td>47.72</td>\n",
       "      <td>38.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>X</td>\n",
       "      <td>19.74</td>\n",
       "      <td>6.43</td>\n",
       "      <td>24.20</td>\n",
       "      <td>25.66</td>\n",
       "      <td>3</td>\n",
       "      <td>25.72</td>\n",
       "      <td>30.81</td>\n",
       "      <td>16.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Y</td>\n",
       "      <td>24.40</td>\n",
       "      <td>8.05</td>\n",
       "      <td>33.42</td>\n",
       "      <td>31.29</td>\n",
       "      <td>7</td>\n",
       "      <td>24.88</td>\n",
       "      <td>25.27</td>\n",
       "      <td>22.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Z</td>\n",
       "      <td>41.40</td>\n",
       "      <td>11.70</td>\n",
       "      <td>44.01</td>\n",
       "      <td>46.35</td>\n",
       "      <td>7</td>\n",
       "      <td>31.62</td>\n",
       "      <td>40.78</td>\n",
       "      <td>31.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   firm     x1     x2     x3     x4  x5     y1     y2     y3\n",
       "0     A  86.13  16.24  48.21  49.69   9  54.53  58.98  38.16\n",
       "1     B  29.26  10.24  41.96  40.65   5  24.69  33.89  26.02\n",
       "2     C  43.12  11.31  38.19  35.03   9  36.41  40.62  28.51\n",
       "3     D  24.96   6.14  24.81  25.15   7  14.94  17.58  16.19\n",
       "4     E  11.62   2.21   6.85   6.37   4   7.81   6.94   5.37\n",
       "5     F  11.88   4.97  18.73  18.04   4  12.59  16.85  12.84\n",
       "6     G  32.64   6.88  28.10  25.45   7  17.06  16.99  17.82\n",
       "7     H  20.79  12.97  54.85  52.07   8  20.29  30.64  33.16\n",
       "8     I  34.40  11.04  38.16  42.40   8  26.13  29.80  26.29\n",
       "9     J  61.74  14.50  49.09  42.92   9  46.42  51.59  35.20\n",
       "10    K  52.92  11.67  39.48  39.64   5  39.80  37.73  30.29\n",
       "11    L  36.00  10.15  37.80  39.52   5  37.84  47.85  25.35\n",
       "12    M  39.20  10.80  41.04  41.12   7  26.48  31.36  26.54\n",
       "13    N  14.60   2.88   9.64  11.14   3  10.31  10.86   7.47\n",
       "14    O   4.29   5.42  21.45  17.27   5  14.39  18.30  14.33\n",
       "15    P  27.25  14.17  56.46  55.26   9  32.94  36.03  38.19\n",
       "16    Q  22.63   4.43  15.40  15.00   2  17.25  20.80  12.07\n",
       "17    R  28.00   7.61  28.73  27.04   9  27.55  38.19  20.44\n",
       "18    S  53.56  13.70  53.04  49.85   7  41.12  43.80  36.54\n",
       "19    T  25.42   9.05  29.69  31.74   4  29.43  42.63  23.34\n",
       "20    U  31.57  10.08  39.34  40.57   6  37.46  51.02  27.44\n",
       "21    V  16.34   5.84  20.89  22.10   4  19.40  25.18  16.52\n",
       "22    W  44.28  14.14  56.70  52.27  11  39.88  47.72  38.97\n",
       "23    X  19.74   6.43  24.20  25.66   3  25.72  30.81  16.54\n",
       "24    Y  24.40   8.05  33.42  31.29   7  24.88  25.27  22.43\n",
       "25    Z  41.40  11.70  44.01  46.35   7  31.62  40.78  31.16"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x9396240>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For the article\n",
    "inputs = pd.DataFrame(inpt_df.iloc[:4,4])\n",
    "outputs = outpt_df.iloc[:4,0:2]\n",
    "l = []\n",
    "for col in inputs.columns:\n",
    "    l.append(outputs.div(inputs[col], axis=0))\n",
    "complete = pd.concat(l, axis=1)\n",
    "complete.columns = ['y1', 'y2']\n",
    "completed = pd.DataFrame(df.iloc[:4, 0]).join(complete)\n",
    "\n",
    "#Plotting the graph\n",
    "p1 = sns.regplot(data = completed, x=\"y1\", y=\"y2\", fit_reg = False, marker=\"o\", color = 'black')\n",
    " \n",
    "# add annotations one by one with a loop\n",
    "for line in range(0,completed.shape[0]):\n",
    "     p1.text(completed.y1[line]+0.2, completed.y2[line], completed.firm[line], horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "\n",
    "p1.set_title('Efficient frontier')\n",
    "\n",
    "plt.plot([0, 4.938],[6.778,6.778], \"r--\")\n",
    "plt.plot([4.938, 6.058889],[6.778, 6.553333], \"r--\")\n",
    "plt.plot([6.058889, 6.058889],[6.553333, 0], \"r--\")\n",
    "plt.plot([0,5.8],[0,6.6] , \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./files/./files/data-2018-09-24.csv')\n",
    "inputss = pd.DataFrame(df.iloc[:,1:3])\n",
    "outputss = pd.DataFrame(df.iloc[:,3])\n",
    "comp = np.array(df.iloc[:, 0])\n",
    "inpt_arr = np.array(inputss)\n",
    "outpt_arr = np.array(outputss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.0000000843334735\n",
      "            Iterations: 5\n",
      "            Function evaluations: 40\n",
      "            Gradient evaluations: 5\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.44444447444563595\n",
      "            Iterations: 5\n",
      "            Function evaluations: 40\n",
      "            Gradient evaluations: 5\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6666666675156382\n",
      "            Iterations: 5\n",
      "            Function evaluations: 41\n",
      "            Gradient evaluations: 5\n",
      "---------------------------\n",
      "\n",
      "Efficient units:\n",
      "'A': 1.0000000843334735\n",
      "\n",
      "\n",
      "Inefficient units:\n",
      "'B': 0.44444447444563595, 'C': 0.6666666675156382\n"
     ]
    }
   ],
   "source": [
    "#headings=['Name','Accession Number','Type','Groups','Description']   # add more as needed, first hit on each is taken\n",
    "#df = pd.DataFrame([]) \n",
    "\n",
    "class DEA(object):\n",
    "    random.seed(5)\n",
    "    def __init__(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Initialize the DEA object with input data\n",
    "        n = number of entities (observations)\n",
    "        m = number of inputs (variables, features)\n",
    "        r = number of outputs\n",
    "        :param inputs: inputs, n x m numpy array\n",
    "        :param outputs: outputs, n x r numpy array\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "\n",
    "        # supplied data\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "        # parameters\n",
    "        self.n = inputs.shape[0]\n",
    "        self.m = inputs.shape[1]\n",
    "        self.r = outputs.shape[1]\n",
    "\n",
    "        # iterators\n",
    "        self.unit_ = range(self.n)\n",
    "        self.input_ = range(self.m)\n",
    "        self.output_ = range(self.r)\n",
    "\n",
    "        # result arrays\n",
    "        self.output_w = np.zeros((self.r, 1), dtype=np.float)  # output weights\n",
    "        self.input_w = np.zeros((self.m, 1), dtype=np.float)  # input weights\n",
    "        self.lambdas = np.zeros((self.n, 1), dtype=np.float)  # unit efficiencies\n",
    "        self.efficiency = np.zeros_like(self.lambdas)  # thetas\n",
    "\n",
    "        # names\n",
    "        self.names = []\n",
    "\n",
    "    def __efficiency(self, unit):\n",
    "        \"\"\"\n",
    "        Efficiency function with already computed weights\n",
    "        :param unit: which unit to compute for\n",
    "        :return: efficiency\n",
    "        \"\"\"\n",
    "\n",
    "        # compute efficiency\n",
    "        denominator = np.dot(self.inputs, self.input_w)\n",
    "        numerator = np.dot(self.outputs, self.output_w)\n",
    "\n",
    "        return (numerator/denominator)[unit]\n",
    "\n",
    "    def __target(self, x, unit):\n",
    "        \"\"\"\n",
    "        Theta target function for one unit\n",
    "        :param x: combined weights\n",
    "        :param unit: which production unit to compute\n",
    "        :return: theta\n",
    "        \"\"\"\n",
    "        in_w, out_w, lambdas = x[:self.m], x[self.m:(self.m+self.r)], x[(self.m+self.r):]  # unroll the weights\n",
    "        denominator = np.dot(self.inputs[unit], in_w)\n",
    "        numerator = np.dot(self.outputs[unit], out_w)\n",
    "\n",
    "        return numerator/denominator\n",
    "\n",
    "    def __constraints(self, x, unit):\n",
    "        \"\"\"\n",
    "        Constraints for optimization for one unit\n",
    "        :param x: combined weights\n",
    "        :param unit: which production unit to compute\n",
    "        :return: array of constraints\n",
    "        \"\"\"\n",
    "\n",
    "        in_w, out_w, lambdas = x[:self.m], x[self.m:(self.m+self.r)], x[(self.m+self.r):]  # unroll the weights\n",
    "        constr = []  # init the constraint array\n",
    "\n",
    "        # for each input, lambdas with inputs\n",
    "        for input in self.input_:\n",
    "            t = self.__target(x, unit)\n",
    "            lhs = np.dot(self.inputs[:, input], lambdas)\n",
    "            cons = t*self.inputs[unit, input] - lhs\n",
    "            constr.append(cons)\n",
    "\n",
    "        # for each output, lambdas with outputs\n",
    "        for output in self.output_:\n",
    "            lhs = np.dot(self.outputs[:, output], lambdas)\n",
    "            cons = lhs - self.outputs[unit, output]\n",
    "            constr.append(cons)\n",
    "\n",
    "        # for each unit\n",
    "        for u in self.unit_:\n",
    "            constr.append(lambdas[u])\n",
    "\n",
    "        return np.array(constr)\n",
    "\n",
    "    def __optimize(self):\n",
    "        \"\"\"\n",
    "        Optimization of the DEA model\n",
    "        Use: http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.linprog.html\n",
    "        A = coefficients in the constraints\n",
    "        b = rhs of constraints\n",
    "        c = coefficients of the target function\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        d0 = self.m + self.r + self.n\n",
    "        # iterate over units\n",
    "        for unit in self.unit_:\n",
    "            # weights\n",
    "            x0 = np.random.rand(d0) - 0.5\n",
    "            x0 = fmin_slsqp(self.__target, x0, f_ieqcons=self.__constraints, args=(unit,))\n",
    "            # unroll weights\n",
    "            self.input_w, self.output_w, self.lambdas = x0[:self.m], x0[self.m:(self.m+self.r)], x0[(self.m+self.r):]\n",
    "            self.efficiency[unit] = self.__efficiency(unit)\n",
    "\n",
    "    def name_units(self, names):\n",
    "        \"\"\"\n",
    "        Provide names for units for presentation purposes\n",
    "        :param names: a list of names, equal in length to the number of units\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "\n",
    "        assert(self.n == len(names))\n",
    "\n",
    "        self.names = names\n",
    "\n",
    "    def fit(self):\n",
    "        global df\n",
    "        \"\"\"\n",
    "        Optimize the dataset, generate basic table\n",
    "        :return: table\n",
    "        \"\"\"\n",
    "\n",
    "        self.__optimize()  # optimize\n",
    "        print(\"---------------------------\\n\")       \n",
    "\n",
    "        efficient = {}\n",
    "        not_efficient = {} \n",
    "        alls = {}\n",
    "        for n, eff in enumerate(self.efficiency):           \n",
    "            if eff >= 1.:\n",
    "                efficient.update({self.names[n]: eff[0]}) \n",
    "            else:\n",
    "                not_efficient.update({self.names[n]: eff[0]})\n",
    "        for n, eff in enumerate(self.efficiency):\n",
    "            alls.update({self.names[n]: eff[0]})\n",
    "        print(\"Efficient units:\")     \n",
    "        print(str(efficient).replace(\"{\",\"\").replace(\"}\", \"\"))       \n",
    "        print(\"\\n\")    \n",
    "        print(\"Inefficient units:\")\n",
    "        print(str(not_efficient).replace(\"{\",\"\").replace(\"}\", \"\"))  \n",
    "        df = df.append(alls, ignore_index = True).T   \n",
    "\n",
    "dea = DEA(inpt_arr, outpt_arr)\n",
    "dea.name_units(comp)\n",
    "dea.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000003588\n",
      "            Iterations: 18\n",
      "            Function evaluations: 650\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.966316603098\n",
      "            Iterations: 14\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.99581214551\n",
      "            Iterations: 20\n",
      "            Function evaluations: 722\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.932140293891\n",
      "            Iterations: 15\n",
      "            Function evaluations: 540\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000444\n",
      "            Iterations: 8\n",
      "            Function evaluations: 288\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.928787206476\n",
      "            Iterations: 9\n",
      "            Function evaluations: 324\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.922897920306\n",
      "            Iterations: 12\n",
      "            Function evaluations: 432\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000001492\n",
      "            Iterations: 10\n",
      "            Function evaluations: 360\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.871090264284\n",
      "            Iterations: 18\n",
      "            Function evaluations: 648\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000015567\n",
      "            Iterations: 17\n",
      "            Function evaluations: 612\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.0000000317\n",
      "            Iterations: 14\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.974436819017\n",
      "            Iterations: 14\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.873789625393\n",
      "            Iterations: 14\n",
      "            Function evaluations: 505\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.984484190762\n",
      "            Iterations: 9\n",
      "            Function evaluations: 324\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.999999999517\n",
      "            Iterations: 6\n",
      "            Function evaluations: 216\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000023\n",
      "            Iterations: 12\n",
      "            Function evaluations: 432\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000005\n",
      "            Iterations: 6\n",
      "            Function evaluations: 217\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.0\n",
      "            Iterations: 9\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.982341644768\n",
      "            Iterations: 18\n",
      "            Function evaluations: 648\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000082\n",
      "            Iterations: 7\n",
      "            Function evaluations: 252\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000023\n",
      "            Iterations: 10\n",
      "            Function evaluations: 360\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000042344\n",
      "            Iterations: 7\n",
      "            Function evaluations: 252\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.982558049119\n",
      "            Iterations: 22\n",
      "            Function evaluations: 793\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.999999999995\n",
      "            Iterations: 6\n",
      "            Function evaluations: 217\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.984999924808\n",
      "            Iterations: 15\n",
      "            Function evaluations: 540\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.956899154838\n",
      "            Iterations: 18\n",
      "            Function evaluations: 649\n",
      "            Gradient evaluations: 18\n",
      "---------------------------\n",
      "\n",
      "Efficient units:\n",
      "'A': 1.0000000358829255, 'E': 1.0000000044424062, 'H': 1.0000000149214712, 'K': 1.0000000316962117, 'J': 1.0000001556692342, 'Q': 1.0000000000484728, 'P': 1.0000000002282767, 'R': 1.0000000000013682, 'U': 1.0000000002335174, 'T': 1.0000000008183176, 'V': 1.0000004234448105\n",
      "\n",
      "\n",
      "Inefficient units:\n",
      "'C': 0.9958121455099681, 'B': 0.9663166030979251, 'D': 0.9321402938915002, 'G': 0.9228979203055871, 'F': 0.9287872064756271, 'I': 0.8710902642835067, 'M': 0.8737896253930998, 'L': 0.9744368190168068, 'O': 0.9999999995173697, 'N': 0.9844841907617617, 'S': 0.9823416447678736, 'W': 0.9825580491186346, 'Y': 0.9849999248084184, 'X': 0.9999999999952303, 'Z': 0.9568991548383973\n"
     ]
    }
   ],
   "source": [
    "#Calculating DEA\n",
    "class DEA(object):\n",
    "\n",
    "    def __init__(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Initialize the DEA object with input data\n",
    "        n = number of entities (observations)\n",
    "        m = number of inputs (variables, features)\n",
    "        r = number of outputs\n",
    "        :param inputs: inputs, n x m numpy array\n",
    "        :param outputs: outputs, n x r numpy array\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "\n",
    "        # supplied data\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "        # parameters\n",
    "        self.n = inputs.shape[0]\n",
    "        self.m = inputs.shape[1]\n",
    "        self.r = outputs.shape[1]\n",
    "\n",
    "        # iterators\n",
    "        self.unit_ = range(self.n)\n",
    "        self.input_ = range(self.m)\n",
    "        self.output_ = range(self.r)\n",
    "\n",
    "        # result arrays\n",
    "        self.output_w = np.zeros((self.r, 1), dtype=np.float)  # output weights\n",
    "        self.input_w = np.zeros((self.m, 1), dtype=np.float)  # input weights\n",
    "        self.lambdas = np.zeros((self.n, 1), dtype=np.float)  # unit efficiencies\n",
    "        self.efficiency = np.zeros_like(self.lambdas)  # thetas\n",
    "\n",
    "        # names\n",
    "        self.names = []\n",
    "\n",
    "    def __efficiency(self, unit):\n",
    "        \"\"\"\n",
    "        Efficiency function with already computed weights\n",
    "        :param unit: which unit to compute for\n",
    "        :return: efficiency\n",
    "        \"\"\"\n",
    "\n",
    "        # compute efficiency\n",
    "        denominator = np.dot(self.inputs, self.input_w)\n",
    "        numerator = np.dot(self.outputs, self.output_w)\n",
    "\n",
    "        return (numerator/denominator)[unit]\n",
    "\n",
    "    def __target(self, x, unit):\n",
    "        \"\"\"\n",
    "        Theta target function for one unit\n",
    "        :param x: combined weights\n",
    "        :param unit: which production unit to compute\n",
    "        :return: theta\n",
    "        \"\"\"\n",
    "        in_w, out_w, lambdas = x[:self.m], x[self.m:(self.m+self.r)], x[(self.m+self.r):]  # unroll the weights\n",
    "        denominator = np.dot(self.inputs[unit], in_w)\n",
    "        numerator = np.dot(self.outputs[unit], out_w)\n",
    "\n",
    "        return numerator/denominator\n",
    "\n",
    "    def __constraints(self, x, unit):\n",
    "        \"\"\"\n",
    "        Constraints for optimization for one unit\n",
    "        :param x: combined weights\n",
    "        :param unit: which production unit to compute\n",
    "        :return: array of constraints\n",
    "        \"\"\"\n",
    "\n",
    "        in_w, out_w, lambdas = x[:self.m], x[self.m:(self.m+self.r)], x[(self.m+self.r):]  # unroll the weights\n",
    "        constr = []  # init the constraint array\n",
    "\n",
    "        # for each input, lambdas with inputs\n",
    "        for input in self.input_:\n",
    "            t = self.__target(x, unit)\n",
    "            lhs = np.dot(self.inputs[:, input], lambdas)\n",
    "            cons = t*self.inputs[unit, input] - lhs\n",
    "            constr.append(cons)\n",
    "\n",
    "        # for each output, lambdas with outputs\n",
    "        for output in self.output_:\n",
    "            lhs = np.dot(self.outputs[:, output], lambdas)\n",
    "            cons = lhs - self.outputs[unit, output]\n",
    "            constr.append(cons)\n",
    "\n",
    "        # for each unit\n",
    "        for u in self.unit_:\n",
    "            constr.append(lambdas[u])\n",
    "\n",
    "        return np.array(constr)\n",
    "\n",
    "    def __optimize(self):\n",
    "        \"\"\"\n",
    "        Optimization of the DEA model\n",
    "        Use: http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.linprog.html\n",
    "        A = coefficients in the constraints\n",
    "        b = rhs of constraints\n",
    "        c = coefficients of the target function\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        d0 = self.m + self.r + self.n\n",
    "        # iterate over units\n",
    "        for unit in self.unit_:\n",
    "            # weights\n",
    "            x0 = np.random.rand(d0) - 0.5\n",
    "            x0 = fmin_slsqp(self.__target, x0, f_ieqcons=self.__constraints, args=(unit,))\n",
    "            # unroll weights\n",
    "            self.input_w, self.output_w, self.lambdas = x0[:self.m], x0[self.m:(self.m+self.r)], x0[(self.m+self.r):]\n",
    "            self.efficiency[unit] = self.__efficiency(unit)\n",
    "\n",
    "    def name_units(self, names):\n",
    "        \"\"\"\n",
    "        Provide names for units for presentation purposes\n",
    "        :param names: a list of names, equal in length to the number of units\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "\n",
    "        assert(self.n == len(names))\n",
    "\n",
    "        self.names = names\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Optimize the dataset, generate basic table\n",
    "        :return: table\n",
    "        \"\"\"\n",
    "\n",
    "        self.__optimize()  # optimize\n",
    "        print(\"---------------------------\\n\")       \n",
    "\n",
    "        m = {}\n",
    "        l = {}         \n",
    "        for n, eff in enumerate(self.efficiency):           \n",
    "            if eff >= 1.:\n",
    "                m.update({self.names[n]: eff[0]}) \n",
    "            else:\n",
    "                l.update({self.names[n]: eff[0]})                 \n",
    "        print(\"Efficient units:\")     \n",
    "        print(str(m).replace(\"{\",\"\").replace(\"}\", \"\"))       \n",
    "        print(\"\\n\")    \n",
    "        print(\"Inefficient units:\")\n",
    "        print(str(l).replace(\"{\",\"\").replace(\"}\", \"\"))         \n",
    "            \n",
    "\n",
    "dea = DEA(inpt_arr,outpt_arr)\n",
    "dea.name_units(comp)\n",
    "dea.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation efficiency by dividing output by input to be used for the graph\n",
    "frst = outpt_df.div(inpt_df.x1, axis = 0)\n",
    "frst.columns = ['y1-x1', 'y2-x1', 'y3-x1']\n",
    "sec = outpt_df.div(inpt_df.x2, axis = 0)\n",
    "sec.columns = ['y1-x2', 'y2-x2', 'y3-x2']\n",
    "th = outpt_df.div(inpt_df.x3, axis = 0)\n",
    "th.columns = ['y1-x3', 'y2-x3', 'y3-x3']\n",
    "fo = outpt_df.div(inpt_df.x4, axis = 0)\n",
    "fo.columns = ['y1-x4', 'y2-x4', 'y3-x4']\n",
    "fi = outpt_df.div(inpt_df.x5, axis = 0)\n",
    "fi.columns = ['y1-x5', 'y2-x5', 'y3-x5']\n",
    "complete = pd.DataFrame(df.iloc[:,0]).join(frst).join(sec).join(th).join(fo).join(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a graph\n",
    "eff = ['y1-x1', 'y2-x1', 'y3-x1', 'y1-x2', 'y2-x2', 'y3-x2',\n",
    "       'y1-x3', 'y2-x3', 'y3-x3', 'y1-x4', 'y2-x4', 'y3-x4', 'y1-x5',\n",
    "       'y2-x5', 'y3-x5']\n",
    "x = [i for i, _ in enumerate(eff)]\n",
    "\n",
    "min_max_range = {}\n",
    "for e in eff:\n",
    "    min_max_range[e] = [complete[e].min(), complete[e].max(), np.ptp(complete[e])]\n",
    "    complete[e] = np.true_divide(complete[e] - complete[e].min(), np.ptp(complete[e]))\n",
    "    \n",
    "fig, axes = plt.subplots(1, len(x)-1, sharey = False, figsize = (50,10), gridspec_kw = {'wspace':0, 'hspace':0})    \n",
    "\n",
    "# Plot each row\n",
    "for i, ax in enumerate(axes):\n",
    "    for idx in complete.index:\n",
    "        ax.plot(x, complete.loc[idx, eff], color='gray')  \n",
    "    ax.set_xlim([x[i], x[i+1]])\n",
    "\n",
    "plt.title(\"Efficiency by a firm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms = ['1', '2', '3']\n",
    "df = pd.DataFrame(firms)\n",
    "output = { 'firms': ['1', '2', '3'],\n",
    "'Sales': [150, 200, 50],\n",
    "'Profit':[200, 210, 90],\n",
    "'Sth':[1, 2, 3]}\n",
    "df1 = pd.DataFrame.from_dict(output)\n",
    "inputs = { 'firms': ['1', '2', '3'],\n",
    "'Salary': [10000, 20000, 500],\n",
    "'employees':[2, 4, 5]}\n",
    "df2 = pd.DataFrame.from_dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete.to_csv('./files/output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
